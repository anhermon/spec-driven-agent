# Testing Guide

This directory contains comprehensive tests for the spec-driven-agent project, including unit tests, integration tests, and test utilities.

## ğŸ“ Test Structure

```
tests/
â”œâ”€â”€ conftest.py                    # Shared fixtures and configuration
â”œâ”€â”€ unit/                          # Unit tests by module
â”‚   â”œâ”€â”€ test_agents/               # Agent tests
â”‚   â”‚   â”œâ”€â”€ test_base_agent.py
â”‚   â”‚   â”œâ”€â”€ test_analyst_agent.py
â”‚   â”‚   â”œâ”€â”€ test_product_manager_agent.py
â”‚   â”‚   â””â”€â”€ test_agent_manager.py
â”‚   â”œâ”€â”€ test_models/               # Data model tests
â”‚   â”‚   â”œâ”€â”€ test_task.py
â”‚   â”‚   â”œâ”€â”€ test_agent.py
â”‚   â”‚   â”œâ”€â”€ test_artifact.py
â”‚   â”‚   â”œâ”€â”€ test_context.py
â”‚   â”‚   â”œâ”€â”€ test_project.py
â”‚   â”‚   â”œâ”€â”€ test_specification.py
â”‚   â”‚   â””â”€â”€ test_workflow.py
â”‚   â”œâ”€â”€ test_core/                 # Core engine tests
â”‚   â”‚   â”œâ”€â”€ test_context_engine.py
â”‚   â”‚   â”œâ”€â”€ test_workflow_orchestrator.py
â”‚   â”‚   â”œâ”€â”€ test_artifact_manager.py
â”‚   â”‚   â”œâ”€â”€ test_state_manager.py
â”‚   â”‚   â”œâ”€â”€ test_symbolic_engine.py
â”‚   â”‚   â””â”€â”€ test_consistency_validator.py
â”‚   â””â”€â”€ test_cli/                  # CLI tests
â”‚       â””â”€â”€ test_main.py
â”œâ”€â”€ integration/                   # Integration tests
â”‚   â”œâ”€â”€ test_api_integration.py
â”‚   â”œâ”€â”€ test_agent_workflow.py
â”‚   â””â”€â”€ test_end_to_end.py
â”œâ”€â”€ fixtures/                      # Test data and fixtures
â”‚   â”œâ”€â”€ sample_data.py
â”‚   â””â”€â”€ mock_responses.py
â””â”€â”€ utils/                         # Test utilities
    â””â”€â”€ test_helpers.py
```

## ğŸš€ Quick Start

### 1. Setup Testing Environment

```bash
# Run the setup script (recommended)
./scripts/setup_testing.sh

# Or manually:
pip install -r requirements.txt
pip install -e ".[dev]"
pre-commit install
```

### 2. Run Tests

```bash
# Run all unit tests
pytest tests/unit/ -v

# Run all integration tests
pytest tests/integration/ -v

# Run all tests with coverage
pytest --cov=spec_driven_agent --cov-report=html

# Run specific test file
pytest tests/unit/test_agents/test_base_agent.py -v

# Run tests matching a pattern
pytest -k "test_agent" -v
```

## ğŸ§ª Test Types

### Unit Tests

Unit tests focus on testing individual components in isolation:

- **Agent Tests**: Test agent functionality, task processing, and capabilities
- **Model Tests**: Test data validation, serialization, and business logic
- **Core Tests**: Test core engine components and workflows
- **CLI Tests**: Test command-line interface functionality

### Integration Tests

Integration tests verify that components work together correctly:

- **API Integration**: Test API endpoints and request/response handling
- **Agent Workflow**: Test multi-agent coordination and task flow
- **End-to-End**: Test complete user scenarios

### Test Utilities

- **TestDataFactory**: Factory for creating test data objects
- **AsyncTestCase**: Base class for async test cases
- **TestAssertions**: Custom assertions for common patterns
- **MockResponse**: Mock HTTP responses for testing

## ğŸ“Š Coverage Requirements

- **Unit Tests**: 90%+ coverage for core business logic
- **Integration Tests**: 70%+ coverage for API endpoints
- **Critical Paths**: 100% coverage for error handling

## ğŸ”§ Pre-commit Hooks

The project uses pre-commit hooks to ensure code quality:

```bash
# Install pre-commit hooks
pre-commit install

# Run all hooks on staged files
pre-commit run

# Run all hooks on all files
pre-commit run --all-files

# Run specific hook
pre-commit run black
```

### Quality Gates

- **Code Formatting**: Black and isort
- **Linting**: Flake8 with project-specific rules
- **Type Checking**: MyPy (optional in pre-commit)
- **Unit Tests**: All tests must pass
- **Coverage**: Minimum 80% coverage requirement
- **Security**: Bandit security checks

## ğŸ¯ Writing Tests

### Test Naming Convention

```python
def test_component_functionality_description():
    """Test description of what is being tested."""
    # Test implementation
```

### Async Test Pattern

```python
@pytest.mark.asyncio
async def test_async_functionality():
    """Test async functionality."""
    result = await some_async_function()
    assert result is not None
```

### Using Fixtures

```python
def test_with_fixture(sample_agent, sample_task):
    """Test using fixtures."""
    result = sample_agent.process_task(sample_task)
    assert result["status"] == "completed"
```

### Mocking

```python
@patch("module.function")
def test_with_mock(mock_function):
    """Test with mocked dependencies."""
    mock_function.return_value = "mocked_result"
    result = function_under_test()
    assert result == "mocked_result"
```

### Test Data Factory

```python
from tests.utils.test_helpers import TestDataFactory

def test_with_factory():
    """Test using the test data factory."""
    task = TestDataFactory.create_task(
        name="Custom Task",
        task_type=TaskType.REQUIREMENTS_GATHERING
    )
    assert task.name == "Custom Task"
```

## ğŸ” Debugging Tests

### Verbose Output

```bash
pytest -v -s  # Verbose with print statements
pytest --tb=long  # Full traceback
pytest --tb=short  # Short traceback
```

### Debugging Specific Tests

```bash
# Run with debugger
pytest --pdb

# Run specific test with debugger
pytest test_file.py::test_function --pdb
```

### Coverage Analysis

```bash
# Generate HTML coverage report
pytest --cov=spec_driven_agent --cov-report=html

# Open coverage report
open htmlcov/index.html
```

## ğŸ“ˆ Performance Testing

### Execution Time

```python
from tests.utils.test_helpers import PerformanceTestMixin

class TestPerformance(PerformanceTestMixin):
    async def test_performance(self):
        """Test that function completes within time limit."""
        result, execution_time = await self.measure_execution_time(
            some_async_function(), max_seconds=5.0
        )
        assert execution_time < 5.0
```

### Memory Usage

```python
def test_memory_usage(self):
    """Test that function doesn't use excessive memory."""
    result = self.assert_memory_usage_acceptable(
        some_function, max_memory_mb=100.0
    )
```

## ğŸš¨ Common Issues

### Import Errors

If you encounter import errors:

1. Ensure you're in the project root directory
2. Activate the virtual environment: `source .venv/bin/activate`
3. Install the package in development mode: `pip install -e .`

### Async Test Issues

For async test issues:

1. Use `@pytest.mark.asyncio` decorator
2. Ensure proper event loop setup in `conftest.py`
3. Use `await` for async function calls

### Pre-commit Hook Failures

If pre-commit hooks fail:

1. Run `pre-commit run --all-files` to see all issues
2. Fix formatting issues: `black .` and `isort .`
3. Fix linting issues: `flake8 .`
4. Fix type issues: `mypy spec_driven_agent/`

## ğŸ“š Best Practices

1. **Test Isolation**: Each test should be independent
2. **Descriptive Names**: Use clear, descriptive test names
3. **Arrange-Act-Assert**: Structure tests with clear sections
4. **Minimal Fixtures**: Keep fixtures focused and minimal
5. **Mock External Dependencies**: Don't rely on external services
6. **Test Edge Cases**: Include error conditions and edge cases
7. **Fast Tests**: Keep tests fast for quick feedback
8. **Clear Assertions**: Use specific, meaningful assertions

## ğŸ”„ Continuous Integration

Tests are automatically run in CI/CD pipelines:

- **Pre-commit**: Runs on every commit
- **Pre-push**: Runs before pushing to remote
- **CI Pipeline**: Runs on pull requests and merges

## ğŸ“ Getting Help

- Check the test output for specific error messages
- Review the test structure and naming conventions
- Consult the pytest documentation: https://docs.pytest.org/
- Check the pre-commit documentation: https://pre-commit.com/

---

**Happy Testing! ğŸ¯** 